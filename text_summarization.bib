
@article{rush_neural_2015,
  title = {A {{Neural Attention Model}} for {{Abstractive Sentence Summarization}}},
  abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
  timestamp = {2017-06-04T09:34:12Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.00685},
  primaryClass = {cs},
  journal = {arXiv:1509.00685 [cs]},
  author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {arXiv\:1509.00685 PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/X7QZCBD8/Rush et al. - 2015 - A Neural Attention Model for Abstractive Sentence .pdf:application/pdf;arXiv.org Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/4P68WK6I/1509.html:text/html},
  groups = {Text Summarization}
}

@misc{knight_new_2017,
  type = {Blog},
  title = {A New {{AI}} Algorithm Summarizes Text Amazingly Well},
  abstract = {Training software to accurately sum up information in documents could have great impact in many fields, such as medicine, law, and scientific research.},
  language = {en},
  timestamp = {2017-06-17T09:03:55Z},
  urldate = {2017-06-17},
  howpublished = {\url{https://www.technologyreview.com/s/607828/an-algorithm-summarizes-lengthy-text-surprisingly-well/}},
  journal = {MIT Technology Review},
  author = {Knight, Will},
  month = may,
  year = {2017},
  file = {Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/ATPSRMNQ/an-algorithm-summarizes-lengthy-text-surprisingly-well.html:text/html},
  groups = {Text Summarization}
}

@misc{paulus_your_2017,
  title = {Your Tl;Dr by an Ai: A Deep Reinforced Model for Abstractive Summarization},
  abstract = {Your TL;DR by an AI: A Deep Reinforced Model for Abstractive Summarization},
  language = {en},
  timestamp = {2017-06-18T12:00:20Z},
  urldate = {2017-06-18},
  howpublished = {\url{https://metamind.io/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization}},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  month = may,
  year = {2017},
  file = {Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/U7VXZCMT/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization.html:text/html},
  groups = {Text Summarization}
}

@article{benikova_bridging_2016,
  title = {Bridging the Gap between Extractive and Abstractive Summaries: {{Creation}} and Evaluation of Coherent Extracts from Heterogeneous Sources},
  abstract = {In our work, we present a corpus of heterogeneous documents for summarization to address the issue that information seekers usually face a range of different types of information sources. A second issue we address, is the summary type, as most manual summaries are abstractive, whereas automatic methods mainly create extractive summaries, which are hard to compare to each other using standard evaluation methods. Therefore, we suggest a multi-step process for creating $\backslash$emph\{coherent extracts\}, which are based on information taken directly from the source documents, but minimally redacted and meaningfully ordered to form a coherent text. Our qualitative and quantitative evaluation show that quantitative results are not sufficient to judge the quality of a summary and that other quality criteria, such as coherence, should also be taken into account. We find that our corpus is of high quality and that it has the potential to bridge the gap between reference corpora of abstracts and automatic methods producing extracts. Our corpus is available to the research community for further development.},
  timestamp = {2017-06-18T12:27:13Z},
  author = {Benikova, Darina and Mieskes, Margot and Meyer, Christian M. and Gurevych, Iryna},
  month = dec,
  year = {2016},
  pages = {1039--1050},
  groups = {Text Summarization}
}

@inproceedings{jones_automatic_1998,
  title = {Automatic {{Summarising}}: {{Factors}} and {{Directions}}},
  shorttitle = {Automatic {{Summarising}}},
  abstract = {Conservative leader Stephen Harper pledged to begin the process of re-building the Canadian Armed Forces in order to strengthen Canadian sovereignty and improve national security. The military is an important, unifying and national institution. Its a bridge to our past, a guardian of our sovereignty, a tool for peacekeeping and a vital contributor to national security, said Harper. The Conservative leader said the Liberals have cut \$20 billion in purchasing power from the Department of National Defence over the past 10 years. Canada now spends less on defence than any other NATO member with a military except Luxembourg. Canadians are proud of our troops, said Harper. But theyre embarrassed by the Liberals sinking subs, crashing choppers, green uniforms and their heavy reliance on the Americans for such basic tasks as long-range troop transport. Harper said the Conservatives military platform is a budgeted, multi-year plan designed to strengthen Canadian sovereignty, improve national security and enhance Canadas standing in the world community. Highlights of the Conservative plan include: Funding: Give the Forces and immediate injection of \$1.2 billion and work to bring Canadas defence spending as a percentage of GDP in line with NATOs European average. Personnel: Gradually increase the Forces regular strength to 80,000 along with simultaneous increases in reserve personnel levels. Army: Provide the Army with strengthened infantry battalion groups, purchase more survivable tanks, and increased army field strength and command capabilities. Air Force: Upgrade the CF-18 fleet, acquire new tactical and heavy lift aircraft as well new maritime helicopters with enhanced multi-mission capabilities. Navy: Acquire new multi-role combatants, new hybrid carriers for helicopter support and lift, and a long-range unmanned air vehicle for maritime surveillance. Coastal Protection: Establish an independent Canadian Coast Guard with more responsibility for coastal policing. Parliamentary Oversight: Enhanced Parliamentary oversight over defence policy and military equipment procurement. At his announcement, Harper took time out to specifically cite the hard work and sacrifices made by Canadian troops in the various roles that they play including peacekeeping, the War on Terror and in responding to national emergencies. The Conservative plan for our nations military reflects a strong commitment to our troops, said Harper. If we are going to send our men and women into harms way, we need to give them the support and tools they need to do the jobs we ask them to do.},
  timestamp = {2017-06-25T09:27:50Z},
  booktitle = {Advances in {{Automatic Text Summarization}}},
  publisher = {{MIT Press}},
  author = {Jones, Karen Sparck},
  year = {1998},
  pages = {1--12},
  file = {Citeseer - Full Text PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/BI7IZK8X/Jones - 1998 - Automatic Summarising Factors and Directions.pdf:application/pdf;Citeseer - Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/RGHMPH94/summary.html:text/html},
  groups = {Text Summarization}
}

@article{erkan_lexrank:_2004,
  title = {{{LexRank}}: {{Graph}}-Based {{Lexical Centrality As Salience}} in {{Text Summarization}}},
  volume = {22},
  issn = {1076-9757},
  shorttitle = {{{LexRank}}},
  abstract = {We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.},
  timestamp = {2017-06-25T09:49:38Z},
  number = {1},
  journal = {J. Artif. Int. Res.},
  author = {Erkan, G{\"u}nes and Radev, Dragomir R.},
  month = dec,
  year = {2004},
  pages = {457--479},
  groups = {Text Summarization}
}

@incollection{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  timestamp = {2017-06-25T10:19:01Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  file = {NIPS Full Text PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/M82FP75D/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshort:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/UPWQKIHT/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html},
  groups = {Text Summarization}
}

@misc{liu_text_2016,
  title = {Text Summarization with {{TensorFlow}}},
  abstract = {Posted by Peter Liu and Xin Pan, Software Engineers, Google Brain Team   Every day, people rely on a wide variety of sources to stay informe...},
  timestamp = {2017-06-25T10:25:33Z},
  urldate = {2017-06-25},
  journal = {Google Research Blog},
  author = {Liu, Peter and Pan, Xing},
  month = aug,
  year = {2016},
  file = {Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/M9UPUXM2/text-summarization-with-tensorflow.html:text/html},
  groups = {Text Summarization}
}

@article{paulus_deep_2017,
  title = {A {{Deep Reinforced Model}} for {{Abstractive Summarization}}},
  abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit "exposure bias" -- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.},
  timestamp = {2017-06-26T15:14:18Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.04304},
  primaryClass = {cs},
  journal = {arXiv:1705.04304 [cs]},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {arXiv\:1705.04304 PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/A329DBD6/Paulus et al. - 2017 - A Deep Reinforced Model for Abstractive Summarizat.pdf:application/pdf;arXiv.org Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/5BQAHHRJ/1705.html:text/html},
  groups = {Text Summarization}
}

@inproceedings{lin_rouge:_2004,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  shorttitle = {{{ROUGE}}},
  timestamp = {2017-06-26T17:57:25Z},
  urldate = {2017-06-26},
  publisher = {{Association for Computational Linguistics}},
  author = {Lin, Chin-Yew},
  year = {2004},
  pages = {74--81},
  file = {Full Text PDF:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/J5BI676Q/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf;Snapshot:/Users/mca/Library/Application Support/Zotero/Profiles/b0y4t97l.default/zotero/storage/8CUWCWH8/W04-1013.html:text/html},
  groups = {Text Summarization}
}


